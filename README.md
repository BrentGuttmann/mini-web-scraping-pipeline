# Mini Web-Scraping Pipeline

## What's This?

This is a simple demo pipeline that demonstrates how to scrape data from different sorts of webpages using python and bash scripting.

This repo is forked from [this template python project](https://github.com/dwd-umd/python-project-template). If you are new to python or need a refresher, then I recommend you first read through the readme of that repo. This will also introduce you to setting up full-featured python3 project in the popular text editor Visual Studio Code (VSCode) with powerful developer tools.

## Quick Start

From a linux-like OS with `git` installed:

```bash
    git clone https://github.com/dwd-umd/miniWebScrapingPipeline.git
    cd miniWebScrapingPipeline
    source _initial_setup.sh
    sh _run_pipeline_1.sh
```

These commands are designed to generate a list of extracted text items in the directory `/pipeline1/data1/extracted-text.txt`.

If you're using a PC then you either need to adapt these scripts (not recommended) or run them from a linux emulator (e.g. using Docker).

## Web Scraping Overview

HTML provides the skeleton of every web site. If you want to systematically extract data from web pages, then you need to get the html on to your computer, and parse through the html syntax with a script.

Whether it's faster/easier to get the data you want by manually copying/pasting, or by writing a script, is a judgement you have to make on the following kinds of consideration:

- How much content is there? The more content there is, the more sense it makes to write a script.
- How consistent is the html? If the html is well-written and consistent, then it will be easy to extract the data you want. If it's all over the place (e.g. Wikipedia, which is generated by all sorts of diverse minds), then you'll be in for a challenge. Another key consideration is whether the html is generated using javascript on the browser (discussed below).
- How much fun will it be to write a script for a given site?

In order to make these sorts of somewhat subjective judgments, you need to visually examine the html on your target web page and figure out essentially how the target website is built before designing your approach.

## Target Webpage for This Repo

This repo provides THREE exemplary pipelines to extract the same ~616 blocks of text (the 'target text') that appear when you hover your mouse over images on the following webpage:

[NASA Solar System](https://solarsystem.nasa.gov/missions/dawn/galleries/images/?page=0&per_page=25
&order=created_at+desc&search=&tags=dawn%3Aceres&condition_1=1%3Ais_in_resource_list&category=51)

This is a non-trivial/instructive page to study because it's not just plain html, rather, it uses javascript to populate its html on the fly with data from an API.

You can tell this page uses javascript by examining it with the Chrome devloper tools. (Chrome has a ton of developer tools and is by some distance the browser that most professionals use when building webpages.) You can inspect the 'raw' html that is first downloaded by the browser by going to `View > Developer > View Source`. You'll notice that this html doesn't have any of the target text.

When this html is first loaded into the browser, it has `<script>` tags fire off javascript. This javascript will call for data from an API and use that data to dynamically inject new html tags into the page body. You can see the effects of this by going to `View > Developer > Inspect Elements` in Chrome. This will open a panel with all of your effective html. If you do a word search for some of the target text, you'll see the divs, etc. that got created by the javascript.

## Pipelines: General Concept

A pipeline is a set of scripts that takes an input data set and sequentially outputs modified data sets. By breaking a pipeline up into multiple steps of input and output files, you make your data-modification process easier to debug and develop.

By contrast, if you put all of your web scraping into a single script then it might suffer from the following:

- You might excessively ping your online data source and risk having them block your requests
- Your development flow might slow down; if you're working on a part of the pipeline towards the end then you'd have to keep running everything from start to finish to debug and develop; with a pipeline you can run from the stage that takes as input your last recorded output file
- Your single script might get long and difficult to read

These pipelines are built with `bash` and `python3`. Bash is used to trigger different "stages" of the pipeline. triggering a stage in the pipeline essentially means running a single `python3` script.

## Three Example Pipelines

The following three pipelines demonstrate different ways to get the same data. (In this particular instance, the second pipeline is "the best" approach; the first and second are provided for completeness.)

### Pipeline 1: Copy/Paste Javascript-Enriched HTML

To operate this pipeline, run `sh _run_pipeline_1.sh` with no arguments.

## Pipeline 3: Trigger JS in Simulated Browser

For this approach you need:

- `geckodriver`. If you're on a Mac, you can install this with `brew install geckodriver`. If you're on linux then there are some instructions [here](https://askubuntu.com/a/871077/896933)
